{
  "name": "Lstm cpd",
  "tagline": "LSTM for change point detection",
  "body": "# LSTM's for change point detection\r\nIn this post, we revisit LSTM's. LSTM's are a form of RNN's with both a hidden state and a cell state that is recurrent through time. This solves mainly the vanishing gradient problem that original RNN's suffered from. \r\nThese two posts use LSTM's for basic tasks, like change point detection and integer addition. With these models, we gain intuition for later use in larger environments and complexer datasets.\r\n\r\n# Data\r\nThe data consists of many signals. In the base class, we have one specific signal. In the other class, we have a change point, where we switch from one signal to another. The task for the LSTM is to detect this change in signal characteristics.\r\n\r\n# Results\r\nThe LSTM's achieves up to 98% accuracy on the task. With this result, we show that LSTM's are able to model different signal characteristics and detect a change between them\r\n\r\n# Improvement\r\nThe LSTM labels the change point as a label over the sequence. We might extand this to using the hidden state at every time sequence to locate the change point. Currently, the final hidden state is mapped to a probability of change-point. We can do this mapping at every time step to locate the change point. In that case, the LSTM has only information on the history. A bi-directional LSTM might fix that particular issue.\r\n\r\nAs always, I am curious to any comments and questions. Reach me at romijndersrob@gmail.com",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}